{% include mathjax %}

## 11. Методи розв'язання крайових задач для звичайних диференційних рівнянь

Почнемо з постановки крайових задач.

1. Нелінійна двоточкова крайова задача:

	\begin{equation}
		\label{eq:11.1}
		\frac{\diff \vec U}{\diff x} = \vec F \left(x, \vec U\right) , \quad a < x < b,
	\end{equation}

	\begin{equation}
		\label{eq:11.2}
		\vec \varphi \left(\vec U(a), \vec U(b)\right) = \vec d,
	\end{equation}

	де $$\vec U = (u_1, \ldots, u_m)^\intercal$$, $$u_k = u_k(x)$$, $$\vec F = (f_1, \ldots, f_m)^\intercal$$, $$f_k = f_k(x, \vec U)$$, $$\vec \varphi = (\varphi_1, \ldots, \varphi_m)^\intercal$$, $$\varphi_k = \varphi_k(\vec U(a), \vec U(b))$$, $$\vec d = (d_1, \ldots, d_m)^\intercal$$, $$d_k$$ &mdash; числа.

2. Лінійна двоточкова крайова задача:

	\begin{equation}
		\label{eq:11.3}
		\frac{\diff \vec U}{\diff x} = A(x) \vec U(x) + \vec F(x),
	\end{equation}

	\begin{equation}
		\label{eq:11.4}
		B_1 \vec U(a) + B_2 \vec U(b) = \vec d,
	\end{equation}

	де $$A(x) = (a_{ij}(x))^m_{i,j=1}$$, $$\vec F = (f_1, \ldots, f_m)^\intercal$$, $$f_k = f_k(x)$$, $$B_1$$, $$B_2$$ &mdash; числові матриці $$m \times m$$, $$\vec d$$ &mdash; числовий вектор.

> **Означення**: Крайові умови \eqref{eq:11.2} і \eqref{eq:11.4} називаються _нероздільними_.

> **Означення**: Часто зустрічаються _розділені крайові умови_. Наприклад, для лінійної задачі:
>
> <a id="eq:11.4'"></a>
> \begin{equation}
> C_1 \vec U(a) = \vec d_1, \quad C_2 \vec U(b) = \vec d_2 \tag{4'},
> \end{equation}
>
> де $$C_1$$ &mdash; $$(m - k) \times m$$-матриця повного рангу, $$C_2$$ &mdash; $$k \times m$$-матриця повного рангу, $$\vec d_1$$ &mdash; $$(m-k)$$-вектор, $$\vec d_2$$ &mdash; $$k$$-вектор.

> **Твердження**: До \eqref{eq:11.3}, [$$(4')$$](#eq:11.4') зводиться крайова задача для рівнянь вищих порідків. 

_Доведення_: Справді, нехай задана крайова задача

\begin{equation}
	\left\\{
		\begin{aligned}
			& u^{(m)}(x) = p_1(x) u^{(m-1)}(x) + \ldots + p_m(x) u(x) + f(x), \newline
			& \alpha_{i,1} u^{(m-1)}(a) + \ldots + \alpha_{i,m} u(a) = \mu_i, \quad i = \overline{1, m - k}, \newline
			& \beta_{i,1} u^{(m-1)}(b) + \ldots + \beta_{i,m} u(b) = \nu_i, \quad i = \overline{1,k}.
		\end{aligned}
	\right.
\end{equation}

Вона зводиться до задачі \eqref{eq:11.3}, [$$(4')$$](#eq:11.4') з

\begin{equation}
	A(x) = \begin{pmatrix}
		0 & 1 & 0 & \ldots & 0 \newline
		0 & 0 & 1 & \ddots & \vdots \newline
		\vdots & \ddots & \ddots & \ddots & 0 \newline
		0 & \ldots & 0 & 0 & 1 \newline
		p_m & p_{m-1} & p_{m -2} & \ldots & p_1
	\end{pmatrix},
\end{equation}

\begin{equation}
	C_1 = (\alpha_{ij})\void_{i = \overline{1, m - k}}^{j = \overline{i, m}}, \quad C_2 = (\beta_{ij})\void_{i = \overline{1, k}}^{j = \overline{i, m}},
\end{equation}

\begin{equation}
	\vec d_1 = (\mu_1, \ldots, \mu_{m - k})^\intercal, \quad \vec d_2 = (\nu_1, \ldots, \nu_k)^\intercal.
\end{equation}

> **Зауваження**: Вважаємо, що всі задачі мають єдині розв'язки. 

Розглянемо методи розв'язування цих задач.

### 11.1. Метод стрільби

Розглянемо крайову задачу з нерозділеними крайовими умовами:

\begin{equation}
	\label{eq:11.1.1}
	\frac{\diff \vec U}{\diff x} = A(x) \vec U(x) + \vec F(x),
\end{equation}

\begin{equation}
	\label{eq:11.1.2}
	B_1 \vec U(a) + B_2 \vec U(b) = \vec d,
\end{equation}

Метод стрільби водить крайову задачу до послідовності з $$m + 1$$ задач Коші, а саме:

\begin{equation}
	\label{eq:11.1.3}
	\frac{\diff \vec Y_0}{\diff x} = A(x) \vec Y_0, \quad \vec Y_0(a) = 0.
\end{equation}

\begin{equation}
	\label{eq:11.1.4}
	\frac{\diff \vec Y_i}{\diff x} = A(x) \vec Y_i(x), \quad \vec Y_u(a) = \vec \delta_i,
\end{equation}

для $$i = \overline{1, m}$$, де $$\delta_i = (\delta_{ij})^m_{j = 1}$$.

> **Означення**: Матриця $$Y(x) = \left( \vec Y_i(x) \right)\void_{i = \overline{1, m}}$$ називається _фундаментальною матрицею_ однорідної системи \eqref{eq:11.1.1}.

Розв'язок \eqref{eq:11.1.1} шукаємо у вигляді:

\begin{equation}
	\label{eq:11.1.5}
	\vec Y(x) = \vec Y_0(x) + \Sum_{i = 1}^m c_i \vec Y_i (x).
\end{equation}

Справді, $$\forall c_i$$ він задовольняє \eqref{eq:11.1.1}, а самі $$c_i$$ знаходяться з \eqref{eq:11.1.2}:

\begin{equation}
	B_1 \left( \vec Y_0 + \Sum_i c_i \vec Y_i(a) \right) + B_2 \left( \vec Y_0(b) + \Sum_i c_i \vec Y_i(b) \right) = \vec d,
\end{equation}

або

\begin{equation}
	\label{eq:11.1.6}
	(B_1 + B_2 Y(b)) \vec c = \vec d - B_2 \vec Y_0(b).
\end{equation}

Розв'язуючи цю СЛАР знаходимо $$c_i$$. За єдиністю $$\vec Y(x) = \vec U(x)$$.

> **Алгоритм А1**:
>
> 1. Розв'язуємо задачу Коші \eqref{eq:11.1.3}, знаходимо $$\vec Y_0(b)$$.
>
> 2. Розв'язуємо $$m$$ задач Коші \eqref{eq:11.1.4}, знаходимо $$Y(b)$$.
>
> 3. Розв'язуємо СЛАР \eqref{eq:11.1.6}, знаходимо $$c_i$$, $$i = \overline{1,n}$$.
>
> 4. $$\vec Y(x) = \vec U(x)$$ знаходимо з \eqref{eq:11.1.5}.

Складність цього алгоритму така ж, як і складність розв'язування $$m + 1$$ задачі Коші.

Якщо крайові умови розділені, тобто

\begin{equation}
	C_1 \vec U(a) = \vec d_1, \quad C_2 \vec U(b) = \vec d_2,
\end{equation}

то можна зменшити кількість задач Кошы, які необхідно розв'язати. Для цього побудуємо вектор $$\vec V_0$$ такий, що 

\begin{equation}
	\label{eq:11.1.7}
	C_1 \vec V_0 = \vec d_1.
\end{equation}

Це завжди можна зробити, оскільки кількість рівнянь менша за кількість невідомих. Далі будуємо $$\vec V_i$$, $$i = \overline{1, k}$$ такі, що

\begin{equation}
	\label{eq:11.1.8}
	C_1 \vec V_i = 0, \quad i = \overline{1, k}.
\end{equation}

Знову ж таки, це можна здійсними бо $$\rang C_1 = m - k$$, тобто не повний.

Після цього всього розв'язуємо задачі Коші:

\begin{equation}
	\label{eq:11.1.9}
	\frac{\diff \vec Y_0}{\diff x} = A \vec Y_0 + \vec F, \quad \vec Y_0(a) = \vec V_0
\end{equation}

\begin{equation}
	\label{eq:11.1.10}
	\frac{\diff \vec Y_i}{\diff x} = A \vec Y_i, \quad \vec Y_i(a) = \vec V_i, \quad i = \overline{1, k}.
\end{equation}

Сталі $$c_i$$  знаходимо з другої крайової умови.

> **Алгоритм А2**:
>
> 1. Розв'язуємо СЛАР \eqref{eq:11.1.7}&ndash;\eqref{eq:11.1.8}.
>
> 2. Розв'язуємо задачу Коші \eqref{eq:11.1.9}.
>
> 3. Розв'язуємо $$k$$ задач Коші \eqref{eq:11.1.10}.
>
> 4. Розв'язуємо СЛАР
>
>    \begin{equation}
>    \label{eq:11.1.11}
>    B_2 \vec Y(b) \equiv C_2 \left(\vec Y_0(b) + \Sum_{i = 1}^k c_i \vec Y(b) \right) = \vec d_2
>    \end{equation}
>
> 5. Розв'язок
>
>    \begin{equation}
>    \label{eq:11.1.12}
>    \vec Y(x) = \vec Y_0(x) + \Sum_{i = 1}^k c_i \vec Y_i(x).
>    \end{equation}

Оскільки для A1 та А2 розв'язок задачі Коші шукається чисельно, то фактично маємо не всю функцію $$\vec Y_i(x)$$ а значення

\begin{equation}
	\vec Y_i(x_n), \quad n = \overline{0, N}, \quad x_n \in [a, b].
\end{equation}

Їх треба запамя'ятовувати щоб розв'язати \eqref{eq:11.1.12}. Цього недоліку можна уникнути<!-- у випадку розділеної крайової задачі -->:

> **Алгоритм А3**:
>
> 1. Розв'язуємо СЛАР \eqref{eq:11.1.7}&ndash;\eqref{eq:11.1.8}.
>
> 2. Розв'язуємо задачу Коші \eqref{eq:11.1.9}.
>
> 3. Розв'язуємо $$k$$ задач Коші \eqref{eq:11.1.10} і запам'ятовуємо лише $$\vec Y_i(x_N) = \vec Y_i(b)$$.
>
> 4. Розв'язуємо СЛАР \eqref{eq:11.1.11}.
>
> 5. Розв'язуємо ще одну задачу Коші:
>
>    \begin{equation}
>    \frac{\diff \vec Y}{\diff x} = A \vec Y, \quad \vec Y(a) = \vec V_0 + \Sum_{i = 1}^k \vec V_i.
>    \end{equation}
> 
> 6. Тоді за формулою \eqref{eq:11.1.12} $$\vec Y(x) = \vec U(x)$$.

> **Зауваження**: Зрозуміло, що &laquo;стріляти&raquo;, тобто починати розв'язувати задачу Коші, треба з того боку, де задано більше крайових умов.

> **Зауваження** (_суттєвий недолік алгоритмів!_) Серед власних значень $$A(x)$$, як правило, є такі, що $$\text{Re} \lambda_i(x) > 0$$. Тоді лінійно незалежні розв'язки задачі Коші наростають експоненціально. Це призводить до наростання похибок заокруглень та погано обумовленої матриці системи \eqref{eq:11.1.6} або \eqref{eq:11.1.11} (розв'язки $$\vec Y_i(x)$$ стають майже лінійно залежні).

Тому $$[a,b]$$ розбивають на проміжки $$[x_{p - 1}, x_p]$$, $$p = \overline{1,M}$$, і розв'язують задачу Коші на підпроміжках, а в кінці $$x = x_p$$ ортогоналізують отримані розв'язки. Зрозуміло, що для $$x = b$$ отримують не $$\vec Y_i(b)$$, а деякі $$\vec W_i(b)$$, які залежать від $$\vec Y_i(b)$$ та відповідних перетворень ортогоналізації. З їх допомогою по $$\vec W_i(b)$$ обчислюють $$\vec Y_i(b)$$ та &laquo;прогоняють&raquo; ці умови для всіх значень 

\begin{equation}
	\vec Y(a) = \vec Y_0(a) + \Sum_i c_i \vec Y_i(a).
\end{equation}

Така ідея метода _ортогональної прогонки Годунова_, що широко застосовується на практиці.

### 11.2. Метод пристрілки

Це метод для розв'язування крайової задачі для нелінійних рівнянь аналогічний методу стрільби.

Розглянемо крайову задачу з розділеними крайовими умовами:

\begin{equation}
	\label{eq:11.2.1}
	\frac{\diff \vec U}{\diff x} = \vec F \left(x, \vec U\right), \quad a < x < b,
\end{equation}

\begin{equation}
	\label{eq:11.2.2}
	u_i(a) = c_i, \quad i = \overline{k + 1, m},
\end{equation}

\begin{equation}
	\label{eq:11.2.3}
	\varphi \left( \vec U(b) \right) = d_i, \quad i = \overline{1,k}.
\end{equation}

При $$x = a$$ невідомі $$k$$ початкових умов $$u_i(a)$$, $$i = \overline{1,k}$$. Будемо їх шукати.

Розв'яжемо задачу Коші:

\begin{equation}
	\left\\{
		\begin{aligned}
			& \frac{\diff \vec Y}{\diff x} = \vec F \left(x, \vec Y\right), \quad a < x < b \newline
			& \vec Y(a) = \vec C = (c_i)^m_{i = 1},
		\end{aligned}
	\right.
\end{equation}

де $$c_i$$, $$i = \overline{1, k}$$ &mdash; невідомі. Їх шукаємо з крайової умови \eqref{eq:11.2.3}:

\begin{equation}
	f_i(c_1, \ldots, c_k) \equiv \varphi_i \left( \vec \varphi(b; c_1, \ldots, c_k) \right) - d_i = 0, \quad i = \overline{1,k}.
\end{equation}

Це система нелінійних рівнянь. Задаємо початкові значення $$c_i^{(0)}$$, $$i = \overline{1,k}$$. За якимось ітераційним методом знаходимо її розв'язок. Найзручніше використовувати метод січних.

Метод пристрілки найбільш прозоро виглядає для $$k = 1$$. У цьому випадку нам необхідно знайти тільки $$c_1$$. Використаємо метод ділення навпіл. Знайдемо $$c_1^{(0)}$$ таке, що

\begin{equation}
	\varphi_1 \left( \vec y \left( b; c_1^{(0)} \right) \right) - d_1 > 0,
\end{equation}

та $$c_1^{(1)}$$ таке, що

\begin{equation}
	\varphi_1 \left( \vec y \left( b; c_1^{(1)} \right) \right) - d_1 < 0.
\end{equation}

Тоді вибираємо 

\begin{equation}
	c_1^{(2)} = \frac{c_1^{(0)} + c_1^{(1)}}{2}.
\end{equation}

З інтервалів $$\left[ c_1^{(0)}, c_1^{(2)} \right]$$, $$\left[ c_1^{(1)}, c_1^{(2)} \right]$$ (можливо кінці в іншому порядку) вибираємо такий, що $$\varphi \left( \vec y \left(b; c_1\right) \right) - d_1$$ змінює знак. Процес продовжуємо до виконання умови

\begin{equation}
	\left| \varphi_1 \left( \vec y \left(b, c_1^{(k)}\right) \right) - d_1 \right| < \varepsilon,
\end{equation}

де $$\varepsilon$$ &mdash; задана точність.

### 11.3. Метод лінеаризації

Розглянемо задачу:

\begin{equation}
	\label{eq:11.3.1}
	\frac{\diff \vec U}{\diff x} = \vec F \left(x, \vec U\right) , \quad a < x < b,
\end{equation}

\begin{equation}
	\label{eq:11.3.2}
	\vec \varphi \left(\vec U(a), \vec U(b)\right) = \vec d,
\end{equation}

Метод лінеаризації для задачі \eqref{eq:11.3.1} це аналог методу Ньютона для систем нелінійних рівнянь. Нехай $$\vec Y_0(x)$$ &mdash; деяке наближення. Побудуємо його уточнення $$\vec Z_0(x)$$ до точного розв'язку $$\vec U(x)$$:

\begin{equation}
	\vec U(x) = \vec Y_0(x) + \vec Z_0(x).
\end{equation}

З \eqref{eq:11.3.1} маємо 

\begin{equation}
	\frac{\diff Z_0}{\diff x} = \Phi_F \left( x, \vec V \right) \vec Z_0(x) + \vec F \left( x, \vec Y_0 \right) - \frac{\diff \vec Y_0}{\diff x}.
\end{equation}

Замінюючи середнє значення $$\vec V(x)$$ на $$\vec Y_0(x)$$ отримаємо лінійне рівняння:

\begin{equation}
	\label{eq:11.3.3}
	\frac{\diff \vec Z_0}{\diff z} = \Phi_F \left( x, \vec Y_0 \right) \vec Z_0 + \vec F \left(x, \vec Y_0\right) - \frac{\diff \vec Y_0}{\diff x}.
\end{equation}

Аналогічно

<a id="eq:11.3.4"></a>
\begin{equation}
	\label{eq:11.3.4}
	\Phi_a \left( \vec Y_0(a), \vec Y_0(b) \right) \vec Z_0(a) + \Phi_b \left( \vec Y_0(a), \vec Y_0(b) \right) \vec Z_0(b) = \vec d - \vec \varphi \left(\vec Y_0(a), \vec Y_0(b)\right),
\end{equation}

де 

- $$\Phi_F = \left( \dfrac{\partial F_i}{\partial u_j} \right)^m_{i,j=1}$$ &mdash; матриця Якобі правої частини $$\vec F \left(x, \vec U\right)$$; 

- $$\Phi_a = \left( \dfrac{\partial \varphi_i}{\partial u_j} (a) \right)^m_{i,j=1}$$, $$\Phi_b = \left( \dfrac{\partial \varphi_i}{\partial u_j} (b) \right)^m_{i,j=1}$$ &mdash; матриці Якобі для $$\vec \varphi \left( \vec U(a), \vec U(b) \right)$$ по крайовим умовам в точках $$x = a$$ та $$x = b$$ відповідно.

Задача \eqref{eq:11.3.3}&ndash;[$$(40)$$](#eq:11.3.4) &mdash; лінійна і розв'язується методом стрільби (з ортогоналізацією). Розв'язавши цю задачу, маємо настуне наближення $$\vec Y_1(x) = \vec Y_0(x) + \vec Z_0(x)$$ . Цей процес продовжуємо до виконання умови точності $$\left\| \vec Z_k(x) \right\| < \varepsilon$$.

**Недоліки** методу:

1. Наявність похідної $$\diff \vec Y_0 / \diff x$$ в правій частині. Оскільки розв'язок задач Коші чисельний, то для її обчислення треба застосовувати формули чисельного диференціювання. Це може привести до великих похибок за рахунок нестійкості задачі чисельного диференціювання.

2. Збіжність залежить від вибору $$\vec Y_0$$.

### 11.4. Метод продовження за параметром

Суттєвим недоліком методу ліанерізації є необхідність задавати хороше початкове наближення та чисельне диференціювання попереднього наближення. Розглянемо метод, який позбавлений цих недоліків.

Розглянемо задачу знаходження вектора $$\vec U(x) = (u_i)^n_{i = 1}$$, що задовольняє умовам:

\begin{equation}
	\label{eq:11.4.1}
	\frac{\diff \vec U}{\diff x} = \vec F \left(x, \vec U\right) , \quad a < x < b,
\end{equation}

\begin{equation}
	\label{eq:11.4.2}
	\vec \varphi \left(\vec U(a), \vec U(b)\right) = \vec d,
\end{equation}

Нехай розв'язок цієї задачі існує та єдиний.

Розв'яжемо задачу Коші

\begin{equation}
	\label{eq:11.4.3}
	\frac{\diff \vec Y}{\diff x} = \vec F \left(x, \vec Y\right), \quad \vec Y(a) = \vec Y_0.
\end{equation}

Вибір $$\vec Y_0$$ здійснимо так, щоб було задовольнялося як можна більша кількість з крайових умов \eqref{eq:11.4.2}. Наприклад, якщо $$\varphi_i \left( \vec U(a), \vec U(b)\right) \equiv u_i(a)$$, то вибираємо $$y_{0,i} = d_i$$.

Обчислимо $$\vec d_0 = \vec \varphi \left( \vec Y(a), \vec Y(b)\right)$$. Якщо $$\vec d_0 \equiv \vec d$$, то $$\vec Y \equiv \vec U$$. Але, як правило, $$\vec d_0 \ne \vec d$$ і тому необхідно уточнювати початкове наближення. Розглянемо параметричну крайову задачу

\begin{equation}
	\label{eq:11.4.4}
	\frac{\diff \vec V}{\diff x} = \vec F \left(x, \vec V\right) , \quad a < x < b,
\end{equation}

\begin{equation}
	\label{eq:11.4.5}
	\vec \varphi \left(\vec V(a), \vec V(b)\right) = \lambda \vec d + (1 - \lambda) \vec d_0,
\end{equation}

яка залежить від параметра $$\lambda$$: $$\vec V = \vec V(x, \lambda)$$. Ясно, що $$\vec V(x,0) = \vec Y(x)$$, а $$\vec V(x, 1) = \vec U$$.

Спробуємо продовжити розв'язок задачі \eqref{eq:11.4.4}&ndash;\eqref{eq:11.4.5} від відомого $$\vec Y(x)$$ до шуканого $$\vec U(x)$$. Для цього продиференціюємо \eqref{eq:11.4.4}&ndash;\eqref{eq:11.4.5} по $$\lambda$$:

\begin{equation}
	\frac{\diff}{\diff x} \frac{\partial \vec V}{\partial \lambda} = \Sum_{j = 1}^n \frac{\partial \vec F}{\partial u_j} \cdot \frac{\partial V_j}{\partial \lambda}, \quad a < x < b,
\end{equation}

\begin{equation}
	\Sum_{j = 1}^n \frac{\partial \vec \varphi}{\partial u_j}(a) \cdot \frac{\partial \vec V}{\partial \lambda}(a) + \Sum_{j = 1}^n \frac{\partial \vec \varphi}{\partial u_j}(b) \cdot \frac{\partial \vec V}{\partial \lambda}(b) = \vec d - \vec d_0,
\end{equation}

Позначимо $$\vec Z = \partial \vec V / \partial \lambda$$. Тоді останню систему можна записати у вигляді:

\begin{equation}
	\label{eq:11.4.6}
	\frac{\diff \vec Z}{\diff x} = \vec \Phi_F \left(x, \vec V \right) \vec Z, \quad a < x < b,
\end{equation}

<a id="eq:11.4.7"></a>
\begin{equation}
	\label{eq:11.4.7}
	\vec \Phi_a \left(\vec V(a), \vec V(b)\right) \vec Z(a) + \vec \Phi_b \left(\vec V(a), \vec V(b)\right) \vec Z(b) = \vec d - \vec d_0,
	\end{aligned}
\end{equation}

\begin{equation}
	\label{eq:11.4.8}
	\frac{\partial \vec V}{\partial \lambda} = \vec Z, \quad \vec V(x, 0) = \vec Y_0
\end{equation}

де 

- $$\Phi_F = \left( \dfrac{\partial F_i}{\partial u_j} \right)^n_{i,j=1}$$ &mdash; матриця Якобі правої частини \eqref{eq:11.4.1}, $$\vec F \left(x, \vec U\right)$$; 

- $$\Phi_a = \left( \dfrac{\partial \varphi_i}{\partial u_j} (a) \right)^n_{i,j=1}$$ &mdash; матриця Якобі лівої частини $$\vec \varphi \left( \vec U(a), \vec U(b) \right)$$ крайової умови \eqref{eq:11.4.2} по першому аргументу $$\vec U(a)$$;

- $$\Phi_b = \left( \dfrac{\partial \varphi_i}{\partial u_j} (b) \right)^n_{i,j=1}$$ &mdash; матриця Якобі лівої частини $$\vec \varphi \left( \vec U(a), \vec U(b) \right)$$ крайової умови \eqref{eq:11.4.2} по другому аргументу $$\vec U(b)$$;

Задача \eqref{eq:11.4.6}&ndash;\eqref{eq:11.4.8} не простіше ніж вихідна задача \eqref{eq:11.4.1}&ndash;\eqref{eq:11.4.2}, а ще й складніша за неї. Спростимо її, застосувавши до задачі Коші \eqref{eq:11.4.8} чисельний метод, наприклад, метод Ейлера:

\begin{equation}
	\vec V^{(k + 1)}(x) = \vec V^{(k)}(x) + \Delta \lambda \vec Z^{(k)}(z), \quad \vec V^{(0)}(x) = \vec Y(x).
\end{equation}

Тут $$\vec V^{(k)} = \vec V(x, \lambda_k)$$, $$\Delta \lambda = \lambda_{k + 1} - \lambda_k$$, $$\lambda_0 = 0$$, $$\lambda_K = 1$$, $$k = \overline{1,K}$$.

Знайдене наближення $$\vec V^{(k + 1)}(x)$$  використовується для знаходження наступного наближення $$\vec Z^{(k + 1)}$$ лінійної крайової задачі \eqref{eq:11.4.6}&ndash;[$$(49)$$](#eq:11.4.7).

Повністю алгоритм розв'язання крайової задачі \eqref{eq:11.4.1}&ndash;\eqref{eq:11.4.2}  цим методом такий:

1. Розв'язуємо задачу Коші \eqref{eq:11.4.3} . Задаємо початкові значення $$\vec V^{(0)}(x)  = \vec Y(x)$$

2. Для $$k = \overline{1,K}$$ розв'язуємо лінійні крайові задачі:

	\begin{equation}
		\frac{\diff \vec Z^{(k)}}{\diff x} = \Phi_F \left( x, \vec V^{(k)} \right) \vec Z^{(k)}, \quad a < x < b,
	\end{equation}

	\begin{equation}
		\vec \Phi_a \left(\vec V^{(k)}(a), \vec V^{(k)}(b)\right) \vec Z^{(k)}(a) + \vec \Phi_b \left(\vec V^{(k)}(a), \vec V^{(k)}(b)\right) \vec Z^{(k)}(b) = \vec d - \vec d_0,
	\end{equation}

3. Продовжуємо розв'язок по параметру $$\lambda$$:

	\begin{equation}
		\vec V^{(k + 1)}(x) = \vec V^{(k)}(x) + \Delta \lambda \vec Z^{(k)}(x).
	\end{equation}
	
4. Шуканий розв'язок $$\vec U(x) \approx \vec V^{(K)}(x)$$.

Лінійні крайові задачі пункту 2 розв'язуються, наприклад, методом стрільби. Для розв'язання задачі Коші \eqref{eq:11.4.8}  можна застосовувати більш точні методи ніж метод Ейлера.
