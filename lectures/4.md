{% include mathjax %}

<!-- MarkdownTOC -->

- [4. Ітераційні методи для систем](#4-ітераційні-методи-для-систем)
	- [4.1. Ітераційні методи розв'язання СЛАР](#41-ітераційні-методи-розвязання-слар)
		- [4.1.1. Метод простої ітерації](#411-метод-простої-ітерації)
		- [4.1.2. Метод Якобі](#412-метод-якобі)
		- [4.1.3. Метод Зейделя](#413-метод-зейделя)
		- [4.1.4. Матрична інтерпретація методів Якобі і Зейделя](#414-матрична-інтерпретація-методів-якобі-і-зейделя)
		- [4.1.5. Однокрокові \(двошарові\) ітераційні методи](#415-однокрокові-двошарові-ітераційні-методи)
		- [4.1.6. Збіжності стаціонарних ітераційних процесів у випадку симетричних матриць](#416-збіжності-стаціонарних-ітераційних-процесів-у-випадку-симетричних-матриць)
		- [4.1.7. Метод верхньої релаксації](#417-метод-верхньої-релаксації)
		- [4.1.8. Методи варіаційного типу](#418-методи-варіаційного-типу)

<!-- /MarkdownTOC -->

<a id="4-ітераційні-методи-для-систем"></a>
## 4. Ітераційні методи для систем

<a id="41-ітераційні-методи-розвязання-слар"></a>
### 4.1. Ітераційні методи розв'язання СЛАР

Література:

- Самарский, Гулин, стор.&nbsp;82&ndash;102: [djvu](../books/samarskyi-gulin-1989.djvu), [pdf](../books/samarskyi-gulin-1989/82-102.pdf);

- Березин, Жидков, том&nbsp;II, стор.&nbsp;54&ndash;67: [djvu](../books/berezin-zhidkov-ii-1962.djvu), [pdf](../books/berezin-zhidkov-ii-1962/54-67.pdf).

Систему

\begin{equation}
	\label{eq:4.1.1}
	A \vec x = \vec b
\end{equation}

зводимо до вигляду

\begin{equation}
	\label{eq:4.1.2}
	\vec x = B \vec x + \vec f.
\end{equation}

Будь яка система

\begin{equation}
	\label{eq:4.1.3}
	\vec x = \vec x - C \cdot (A \vec x - \vec b)
\end{equation}

має вигляд \eqref{eq:4.1.2} і при $$\det C \ne 0$$ еквівалентна системі \eqref{eq:4.1.1}. Наприклад, для $$C = \tau \cdot E$$:

$$
\vec x = \vec x - \tau \cdot (A \vec x - \vec b). \tag{3'}
$$

<a id="411-метод-простої-ітерації"></a>
#### 4.1.1. Метод простої ітерації

Цей метод застосовується до рівняння \eqref{eq:4.1.2}

\begin{equation}
	\label{eq:4.1.4}
	\vec x^{(k + 1)} = B \vec x^{(k)} + \vec f,
\end{equation}

де $$\vec x^{(0)}$$ &mdash; початкове наближення, задано.

Ітераційний процес збігається, тобто $$\left\| \vec x^{(k)} - \vec x\right\| \xrightarrow[k\to\infty]{} 0$$, якщо

\begin{equation}
	\label{eq:4.1.5}
	\|B\| \le q < 1.
\end{equation}

При цьому має місце оцінка

\begin{equation}
	\label{eq:4.1.6}
	\left\|\vec x^{(n)} - \vec x\right\| \le \dfrac{q^n}{1-q}\cdot\left\|\vec x^{(1)} - \vec x^{(0)}\right\|.
\end{equation}

<a id="412-метод-якобі"></a>
#### 4.1.2. Метод Якобі

Припустимо $$\forall i$$: $$a_{i,i} \ne 0$$. Зведемо систему \eqref{eq:4.1.1} до вигляду

$$
x_i = -\Sum_{j = 1}^{i - 1} \dfrac{a_{i,j}}{a_{i,i}} \cdot x_j - \Sum_{j = i + 1}^n \dfrac{a_{i,j}}{a_{i,i}} \cdot x_j + \dfrac{b_i}{a_{i,i}},
$$

де $$i = \overline{1, n}$$.

Ітераційний процес запишемо у вигляді

\begin{equation}
	\label{eq:4.1.7}
	x_i^{(k+1)} = -\Sum_{j=1}^{i-1} \dfrac{a_{i,j}}{a_{i,i}} \cdot x_j^{(k)} - \Sum_{j=i+1}^n \dfrac{a_{i,j}}{a_{i,i}} \cdot x_j^{(k)} + \dfrac{b_i}{a_{i,i}},
\end{equation}

де $$k = 0,1,\ldots$$, а $$i=\overline{1,n}$$.

Ітераційний процес збігається до розв'язку, якщо виконується умова

$$
\forall i: \Sum_{\substack{j = 1 \\ i \ne j}}^n |a_{i,j}| \le |a_{i,i}|.
$$

Це умова діагональної переваги матриці $$A$$. Якщо ж

\begin{equation}
	\label{eq:4.1.8}
	\forall i: \Sum_{\substack{j = 1 \\ i \ne j}}^n |a_{i,j}| \le q\cdot|a_{i,i}|, \quad 0 \le q < 1.
\end{equation}

то має місце оцінка точності:

$$
\|\vec x^{(n)} - \vec x\| \le \dfrac{q^n}{1-q}\cdot\|\vec x^{(0)}-\vec x\|.
$$

<a id="413-метод-зейделя"></a>
#### 4.1.3. Метод Зейделя

В компонентному вигляді ітераційний метод Зейделя записується так:

\begin{equation}
	\label{eq:4.1.9}
	x_i^{(k+1)} = -\Sum_{j=1}^{i-1} \dfrac{a_{i,j}}{a_{i,i}} \cdot x_j^{(k+1)} - \Sum_{j=i+1}^n \dfrac{a_{i,j}}{a_{i,i}} \cdot x_j^{(k)} + \dfrac{b_i}{a_{i,i}},
\end{equation}

де $$k = 0,1,\ldots$$, а $$i=\overline{1,n}$$.

На відміну від методу Якобі на $$k$$-му-кроці попередні компоненти розв'язку беруться з $$(k+1)$$-ої ітерації.

Достатня умова збіжності методу Зейделя &mdash; $$A^\intercal = A > 0$$.

<a id="414-матрична-інтерпретація-методів-якобі-і-зейделя"></a>
#### 4.1.4. Матрична інтерпретація методів Якобі і Зейделя

Подамо матрицю $$A$$ у вигляді 

$$
A = A_1 + D + A_2,
$$

де $$A_1$$ &mdash; нижній трикутник матриці $$A$$, $$A_2$$ &mdash; верхній трикутник матриці $$A$$, $$D$$ &mdash; її діагональ. Тоді систему \eqref{eq:4.1.1} запишемо у вигляді 

$$
D \vec x = A_1 \vec x + A_2 \vec x + \vec b,
$$

або

$$
\vec x = D^{-1} A_1 \vec x + D^{-1} A_2 \vec x + D^{-1} \vec b,
$$
 
Матричний запис методу Якобі:

$$
\vec x^{(k+1)} = D^{-1} A_1 \vec x^{(k)} + D^{-1} A_2 \vec x^{(k)} + D^{-1} \vec b,
$$

методу Зейделя:

$$
\vec x^{(k+1)} = D^{-1} A_1 \vec x^{(k+1)} + D^{-1} A_2 \vec x^{(k)} + D^{-1} \vec b,
$$

Необхідна і достатня умова збіжності методу Якобі: всі корені рівняння 

$$
\det(D + \lambda(A_1 + A_2 )) = 0
$$

по модулю більше 1.

Необхідна і достатня умова збіжності методу Зейделя: всі корені рівняння 

$$
\det(A_1 + D + \lambda A_2) = 0
$$
по модулю більше 1.

<a id="415-однокрокові-двошарові-ітераційні-методи"></a>
#### 4.1.5. Однокрокові (двошарові) ітераційні методи

Канонічною формою однокрокового ітераційного методу розв'язку СЛАР є його запис у вигляді

\begin{equation}
	\label{eq:4.1.10}
	B_k \dfrac{\vec x^{(k+1)} - \vec x^{(k)}}{\tau_{k+1}} + A \vec x^{(k)} = \vec b,
\end{equation}

Тут $$\{B_k\}$$ &mdash; послідовність матриць (пере-обумовлюючі матриці), що задають ітераційний метод на кожному кроці; $$\{\tau_{k+1}\}$$ &mdash; ітераційні параметри.

Якщо $$B_k = E$$, то ітераційний процес називається _явним_

$$
\vec x^{(k+1)} = \vec x^{(k)} - \tau_{k+1} \left(A \vec x^{(k)} + \vec b\right).
$$

Якщо $$B_k \ne E$$, то ітераційний процес називається _неявним_

$$
B_k \vec x^{(k+1)} = F^k.
$$

У цьому випадку на кожній ітерації необхідно розв'язувати СЛАР.

Якщо $$\tau_{k+1} \equiv \tau$$, $$B_k \equiv B$$, то ітераційний процес називається _стаціонарним_; інакше &mdash; _нестаціонарним_.

Методам, що розглянуті вище відповідають:

- методу простої ітерації: $$B_k = E$$, $$\tau_{k+1} = \tau$$;

- методу Якобі: $$B_k = D$$, $$\tau_{k+1} = 1$$;

- методу Зейделя: $$B_k = D + A_1$$, $$\tau_{k+1} = 1$$.

<a id="416-збіжності-стаціонарних-ітераційних-процесів-у-випадку-симетричних-матриць"></a>
#### 4.1.6. Збіжності стаціонарних ітераційних процесів у випадку симетричних матриць

Розглянемо випадок симетричних матриць $$A^\intercal=A$$ і стаціонарний ітераційний процес $$B_k \equiv E$$, $$\tau_{k+1} \equiv \tau$$.

Нехай для $$A$$ справедливі нерівності

\begin{equation}
	\label{eq:4.1.11}
	\gamma_1 E \le A \le \gamma_2 E, \quad \gamma_1, \gamma_2 > 0.
\end{equation}

Тоді при виборі $$\tau = \tau_0 = \frac{2}{\gamma_1 + \gamma_2}$$ ітераційний процес збігається. Найбільш точним значенням $$\gamma_1$$, $$\gamma_2$$ при яких виконуються обмеження \eqref{eq:4.1.11} є $$\gamma_1 = \min \lambda_i(A)$$, $$\gamma_2 = \max \lambda_i(A)$$. Тоді 

$$
q = q_0 = \dfrac{\gamma_2 - \gamma_1}{\gamma_2 + \gamma_1} = \dfrac{1-\xi}{1+\xi}, \quad \xi = \dfrac{\gamma_1}{\gamma_2}.
$$

(Зауважимо, що аналогічно обчислюється $$q$$ і для методу релаксації розв'язання нелінійних рівнянь, де $$\gamma_1 = m = \min \vert f'(x)\vert$$, $$\gamma_2 = M_1 = \max\vert f'(x)\vert$$) і справедлива оцінка

$$
\left\|\vec x^{(n)} - \vec x\right\| \le \dfrac{q^n}{1-q} \cdot \left\|\vec x^{(0)} - \vec x\right\|.
$$

Явний метод з багатьма параметрами $$\{\tau_k\}$$:

$$
B \equiv E, \quad \{\tau_k\}: \Min_\tau q(\tau), \quad n=n(\epsilon)\to\min,
$$

які обчислюються за допомогою нулів багаточлена Чебишова, називаються ітераційним методом з чебишевським набором параметрів.

<a id="417-метод-верхньої-релаксації"></a>
#### 4.1.7. Метод верхньої релаксації

Узагальненням методу Зейделя є метод верхньої релаксації: 

$$
(D + \omega A_1) \cdot\dfrac{\vec x^{(k+1)} + \vec x^{(k)}}{\omega} + A \vec x^{(k)} = \vec b,
$$

де $$D$$ &mdash; діагональна матриця з елементами $$a_{i,i}$$ по діагоналі. $$\omega > 0$$ &mdash; заданий числовий параметр.

Тепер $$B = D + \omega A_1$$, $$\tau = \omega$$. Якщо $A^\intercal = A > 0$$, то метод верхньої релаксації збігається при умові $$0 < \omega < 2$$. Параметр підбирається експериментально з умови мінімальної кількості ітерацій. 

<a id="418-методи-варіаційного-типу"></a>
#### 4.1.8. Методи варіаційного типу

До цих методів відносяться: метод мінімальних нев'язок, метод мінімальних поправок, метод найшвидшого спуску, метод спряжених градієнтів. Вони дозволяють обчислювати наближення без використання апріорної інформації про $$\gamma_1$$, $$\gamma_2$$ в \eqref{eq:4.1.11}.

Нехай $$B = E$$. Для методу мінімальних нев'язок параметри $$\tau_{k+1}$$ обчислюються з умови 

$$
\begin{aligned}
	\left\|\vec r^{(k+1)}\right\|^2 &= \left\|\vec r^{(k)}\right\|^2 - 2\tau_{k+1}\cdot\left(\vec r^{(k)}, A\vec r^{(k)}\right) + \newline
	&+ \tau_{k+1}^2 \cdot\left\|A\vec r^{(k)}\right|^2 \to \min.
\end{aligned}
$$

Тому

$$
\tau_{k+1} = \dfrac{\left(A\vec r^{(k)}, \vec r^{(k)}\right) }{\left\|\vec r^{(k)}\right\|^2},
$$

де $$\vec r^{(k)} = A \vec x^{(k)} - \vec b$$ &mdash; нев'язка.

Умова для завершення ітераційного процесу: 

$$
\left\|\vec r^{(n)}\right\| < \epsilon.
$$

Швидкість збіжності цього методу співпадає із швидкістю методу простої ітерації з одним оптимальним параметром $$\tau_0 = \frac{2}{\gamma_1+\gamma_2}$$.

Аналогічно будуються методи з $$B \ne E$$. Матриця $$B$$ називається переобумовлювачем і дозволяє підвищити швидкість збіжності ітераційного процесу. Його вибирають з умов 

- легко розв'язувати СЛАР $$B \vec x^{(k)} = F_k$$ (діагональний, трикутній, добуток трикутніх та інше); 

- зменшення числа обумовленості матриці $$B^{-1}A$$ у порівнянні з $$A$$.

\subsection{Методи розв'язання нелінійних систем}

Розглянемо систему рівнянь
\[ \left\{ \begin{aligned} & f_1(x_1, \ldots, x_n) = 0, \\ & \ldots \\ & f_n(x_1,\ldots,x_n) = 0. \end{aligned} \right. \]

Перепишемо її у векторному вигляді: 
\begin{equation}
	\label{eq:4.13}
	\vec f(\vec x) = 0.
\end{equation}

\subsubsection{Метод простої ітерації}

В цьому методі рівняння (\ref{eq:4.13}) зводиться до еквівалентного вигляду
\begin{equation}
	\label{eq:4.14}
	\vec x = \vec \Phi(\vec x).
\end{equation}

Ітераційний процес представимо у вигляді:
\begin{equation}
	\label{eq:4.15}
	\vec x^{(k+1)} = \vec \Phi\left(\vec x^{(k)}\right).
\end{equation}
початкове наближення $\vec x^{(0)}$ -- задано. \\

Нехай оператор $\vec \Phi$ визначений на множині $H$. За теоремою про стискуючі відображення ітераційний процес (\ref{eq:4.15}) сходиться, якщо виконується умова
\begin{equation}
	\label{eq:4.16}
	\left\| \vec \Phi(\vec x) - \vec \Phi(\vec y) \right\| \le q \cdot \|\vec x - \vec y\|, \quad 0 < q < 1, 
\end{equation}
або
\begin{equation}
	\label{eq:4.17}
	\left\| \vec \Phi'(\vec x)\right\| \le q < 1, 
\end{equation}
де $\vec x\in U_r$, $\vec \Phi'(\vec x) = \left(\frac{\partial \phi_i}{\partial x_j}\right)_{i,j=1}^n$. Для похибки справедлива оцінка
\[ \left\| \vec x^{(m)} - \vec x\right\| \le \dfrac{q^n}{1 - q} \cdot\left\|\vec x^{(0)} - \vec x\right\|.\]

Частинним випадком методу простої ітерації є метод релаксації для рівняння (\ref{eq:4.13}):
\[ \vec x^{(k+1)} = \vec x^{(k)} - \tau \cdot \vec F\left(\vec x^{(k)}\right), \]
де $\tau < \frac{2}{\left\|\vec F'(\vec x)\right\|}$.

\subsubsection{Метод Ньютона}

Розглянемо рівняння
\[ \vec F(\vec x) = 0. \]

Представимо його у вигляді
\begin{equation}
	\label{eq:4.18}
	\vec F\left(\vec x^{(k)}\right) + \vec F'\left(\vec \xi^{(k)}\right)\cdot\left(\vec x - \vec x^{(k)}\right) = 0,
\end{equation}
де $\vec \xi^{(k)} = \vec x^{(k)} + \theta_k \cdot \left(\vec x^{(k)} - \vec x\right)$, $0 < \theta_k < 1$. Тут  $\vec F'(\vec x) = \left(\frac{\partial f_i}{\partial x_j}\right)_{i,j=1}^n$ -- матриця Якобі для $\vec F(\vec x)$. Можемо наближено вважати $\vec \xi^{(k)} \approx \vec x^{(k)}$. Тоді з (\ref{eq:4.18}) матимемо
\begin{equation}
	\label{eq:4.19}
	\vec F\left(\vec x^{(k)}\right) + \vec F'\left(\vec x^{(k)}\right) \cdot\left(\vec x^{(k+1)} - \vec x^{(k)}\right) = 0.
\end{equation}
Ітераційний процес представимо у вигляді:
\begin{equation}
	\label{eq:4.20}
	\vec x^{(k+1)} = \vec x^{(k)} - \vec F'\left(\vec x^{(k)}\right)^{-1} \cdot \vec F\left(\vec x^{(k)}\right). 
\end{equation}

Для реалізації методу Ньютона потрібно, щоб існувала обернена матриця \[\vec F'\left(\vec x^{(k)}\right)^{-1}. \]

Можна не шукати обернену матрицю, а розв'язувати на кожній ітерації СЛАР
\begin{equation}
	\label{eq:4.21}
	\left\{
		\begin{aligned}
			& A_k \vec z^{(k)} = \vec F\left(\vec x^{(k)}\right), \\
			& \vec x^{(k + 1)} = \vec x^{(k)} - \vec z^{(k)},
		\end{aligned}
		\quad k=0,1,2,\ldots
	\right.
\end{equation}
де $\vec x^{(0)}$ -- задано, а матриця $A_k = \vec F'\left(\vec x^{(k)}\right)$. \\

Метод має квадратичну збіжність, якщо добре вибрано початкове наближення. Складність методу (при умові використання методу Гаусса розв'язання СЛАР (\ref{eq:4.21}) на кожній ітерації $Q_n = \frac23 n^3+O(n^2)$, де $n$ -- розмірність системи (\ref{eq:4.13}).

\subsubsection{Модифікований метод Ньютона}

Ітераційний процес має вигляд :
\[ \vec x^{(k+1)} = \vec x^{(k)} - \vec F'\left(\vec x^{(0)}\right)^{-1} \cdot\vec F\left(\vec x^{(k)}\right). \]

Тепер обернена матриця обчислюється тільки на нульовій ітерації. На інших -- обчислення нового наближення зводиться до множення матриці $A_0 = \vec F'\left(\vec x^{(0)}\right)^{-1}$ на вектор $\vec F\left(\vec x^{(k)}\right)$ та додавання до $\vec x^{(k)}$. \\

Запишемо метод у вигляді системи лінійних рівнянь (аналог (\ref{eq:4.21}))
\begin{equation}
	\label{eq:4.22}
	\left\{
		\begin{aligned}
			& A_0 \vec z^{(k)} = \vec F\left(\vec x^{(k)}\right), \\
			& \vec x^{(k + 1)} = \vec x^{(k)} - \vec z^{(k)},
		\end{aligned}
		\quad k=0,1,2,\ldots
	\right.
\end{equation}
Оскільки матиця $A_0$ розкладається на трикутні (або обертається) один раз, то складність цього методу на одній ітерації (окрім нульової) $Q_n = O(n^2)$. Але цей метод має лінійну швидкість збіжності. \\

Можливе циклічне застосування модифікованого методу Ньютона, тобто коли обернену матрицю похідних шукаємо та обертаємо через певне число кроків ітераційного процесу. \\

\begin{problem}
	Побудувати аналог методу січних для систем нелінійних рівнянь.
\end{problem}

[Назад до лекцій](README.md)

[Назад на головну](../README.md)
